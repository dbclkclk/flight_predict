{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6765a11-6c9d-4bfb-a8be-52377357ac3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yellowbrick in /opt/anaconda3/lib/python3.11/site-packages (1.5)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from yellowbrick) (3.8.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from yellowbrick) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from yellowbrick) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/anaconda3/lib/python3.11/site-packages (from yellowbrick) (1.26.4)\n",
      "Requirement already satisfied: cycler>=0.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from yellowbrick) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.0.0->yellowbrick) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.0.0->yellowbrick) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install yellowbrick\n",
    "!pip install xgboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OrdinalEncoder, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, root_mean_squared_error\n",
    "import statsmodels.api as sm \n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import randint, uniform\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd146e5c-8ee4-4538-8cf2-b3aca345f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_excel(\"./Flight_Fare.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e5ea70-2b20-409a-be68-86fa85a21c04",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853c895b-8a9a-426b-b7d7-b80a694a5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c1376f-06ac-40f0-a2bf-4b9a6a43589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9f8e1-148f-4672-b711-c4937e3da8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af8cb4-5bc2-4a72-a4ae-7a096ce53bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f755bc-e9b4-49f4-93a7-70b51c3e3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6530f58-0fb4-4984-89cd-a318faecf2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf7264-3c23-4817-a068-46d25237448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d03a3-d8dd-416c-8c08-a2a25cdaa979",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Additional_Info'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9908cd2-f2b8-42f2-988c-a0f3ad84466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Airline'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63675e9-02f9-4c4e-a73a-f59c4080af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42b10b-ea56-48c8-b771-114908a51333",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Destination'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a51a0c-e874-46ae-a4a0-42ecdd72f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Total_Stops'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b16165-f855-4107-bc88-b1bf2cde57c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Airline'] == 'Jet Airways Business']['Additional_Info'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd31295b-e144-4b49-89e5-537891cf8ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['Airline'] == \"Multiple carriers Premium economy\", 'Additional_Info'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf9a8df-bc1c-4f84-9cc8-ae574ee78bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['Airline'] == 'Vistara Premium economy', 'Additional_Info'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316825a7-66c7-4412-bae2-fea95b3e8303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price Distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(data['Price'], bins=30, kde=True, color='skyblue')\n",
    "plt.title('Price Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ae071-4ce2-4991-981f-c0a5550b36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "contineous_features = [\"Duration\"]\n",
    "discrete_features = []\n",
    "ordinal_features = [\"Dep_Time\", \"Arrival_Time\", \"Total_Stops\", \"Date_of_Journey\"]\n",
    "nominal_features = [\"Airline\", \"Source\", \"Destination\", \"Route\", \"Additional_Info\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f47b80-3e8e-4642-b9cc-accbeff33700",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbeaf03-cac3-4686-9e9e-e885b9f58b51",
   "metadata": {},
   "source": [
    "## Remove Redundant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41454243-3df7-42af-971d-bfd8f540fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Route not needed since it's captured in Total_Stops and Arrival and Departure features\n",
    "# Remove Arrival_Time as this is capture from Departure Time and Duration\n",
    "data.drop([\"Route\", \"Arrival_Time\"], axis=1, inplace=True)\n",
    "ordinal_features.remove(\"Arrival_Time\")\n",
    "nominal_features.remove(\"Route\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86fe661-d572-44a4-8ff5-4f51217020f7",
   "metadata": {},
   "source": [
    "## Data with duplicate values because of casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa5a0c4-6e81-4f6d-a7ec-4390360164a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"Airline\", \"Source\", \"Destination\", \"Total_Stops\", \"Additional_Info\"]\n",
    "for col in categorical_columns:\n",
    "    data[col] = data[col].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d75e6f-5e92-4790-8832-aec7a158de97",
   "metadata": {},
   "source": [
    "## Change Duplicate Records to the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c54de-5db9-4d38-ace0-fe8c0c920399",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['Airline'] == 'jet airways business', 'Additional_Info'] = 'business class'\n",
    "data.loc[data['Airline'] == 'jet airways business', 'Airline'] = 'jet airways'\n",
    "data.loc[data['Airline'] == \"multiple carriers premium economy\", 'Additional_Info'] = 'premium economy class'\n",
    "data.loc[data['Airline'] == \"multiple carriers premium economy\", 'Airline'] = 'multiple carriers'\n",
    "data.loc[data['Airline'] == 'vistara premium economy', 'Additional_Info'] = 'premium economy class'\n",
    "data.loc[data['Airline'] == 'vistara premium economy', 'Airline'] = 'vistara'\n",
    "data.loc[data['Destination'] == 'new delhi', 'Destination'] = 'delhi'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e9bfa-8c04-4575-ac20-914dc009c531",
   "metadata": {},
   "source": [
    "## Impute Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447b6c3-7bc2-4aff-9707-df82cb8d3c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Total_Stops'] = data['Total_Stops'].fillna(data['Total_Stops'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839528cb-1d9e-42c9-bd0f-27e8fb1f69b2",
   "metadata": {},
   "source": [
    "## Encode Ordinal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c50dac-8602-4a32-ba78-edd1f8ac9daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder(categories=[[\"non-stop\", \"1 stop\", \"2 stops\", \"3 stops\", \"4 stops\"]])\n",
    "data['Total_Stops'] = encoder.fit_transform(data[['Total_Stops']])\n",
    "data['Total_Stops'] = data['Total_Stops'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf787855-45c4-4467-9227-b4a674a1070b",
   "metadata": {},
   "source": [
    "## Date Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8edb7f-6bee-4e29-b9c2-3980a89d1e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_duration(duration_str):\n",
    "    \"\"\"Extracts hours and minutes from a duration string.\n",
    "\n",
    "    Args:\n",
    "        duration_str: The duration string in the format \"HHh MMm\" or \"HHh\" or \"MMm\".\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the number of hours and minutes.\n",
    "    \"\"\"\n",
    "    hours = 0\n",
    "    minutes = 0\n",
    "    if 'h' in duration_str:\n",
    "        hours_str, rest = duration_str.split('h')\n",
    "        hours = int(hours_str)\n",
    "    if 'm' in duration_str:\n",
    "        if 'h' in duration_str:\n",
    "            minutes_str = rest.strip('m') \n",
    "        else:\n",
    "            minutes_str = duration_str.strip('m')\n",
    "        minutes = int(minutes_str)\n",
    "    return hours, minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc28508-b10a-42cf-acea-dad803ac9f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date_of_Journey'] = pd.to_datetime(data['Date_of_Journey'], format='%d/%m/%Y')\n",
    "# Extract day, month, year\n",
    "data['Journey_day'] = data['Date_of_Journey'].dt.day\n",
    "data['Journey_month'] = data['Date_of_Journey'].dt.month\n",
    "data['Journey_year'] = data['Date_of_Journey'].dt.year\n",
    "\n",
    "ordinal_features.extend([\"Journey_day\", \"Journey_month\", \"Journey_year\"])\n",
    "\n",
    "# Extract day of the week\n",
    "data['Journey_day_of_week'] = data['Date_of_Journey'].dt.day_name() \n",
    "nominal_features.extend(['Journey_day_of_week'])\n",
    "\n",
    "data['Dep_Time'] = pd.to_datetime(data['Dep_Time'], format='%H:%M')\n",
    "\n",
    "# Extract hour and minute from 'Dep_Time'\n",
    "data['Dep_hour'] = pd.to_datetime(data['Dep_Time']).dt.hour\n",
    "data['Dep_min'] = pd.to_datetime(data['Dep_Time']).dt.minute\n",
    "ordinal_features.extend(['Dep_hour', 'Dep_min'])\n",
    "\n",
    "# Extract hours and minutes from 'Duration'\n",
    "data['Duration_hours'], data['Duration_mins'] = zip(*data['Duration'].apply(extract_duration))\n",
    "\n",
    "# Convert 'Duration_hours' and 'Duration_mins' to numeric\n",
    "data['Duration_hours'] = data['Duration_hours'].astype(int)\n",
    "data['Duration_mins'] = data['Duration_mins'].astype(int)\n",
    "contineous_features.extend(['Duration_hours', 'Duration_mins'])\n",
    "    \n",
    "# Calculate total duration in minutes\n",
    "data['Duration_total_mins'] = data['Duration_hours'] * 60 + data['Duration_mins'] \n",
    "contineous_features.extend(['Duration_total_mins'])\n",
    "\n",
    "# Drop original columns\n",
    "data.drop(['Date_of_Journey', 'Dep_Time', 'Duration', 'Duration_hours', 'Duration_mins'], axis=1, inplace=True) \n",
    "ordinal_features.remove('Date_of_Journey')\n",
    "ordinal_features.remove(\"Dep_Time\")\n",
    "contineous_features.remove('Duration')\n",
    "contineous_features.remove(\"Duration_hours\")\n",
    "contineous_features.remove('Duration_mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e056e442-8418-43d2-8bbe-d302e8de8fd4",
   "metadata": {},
   "source": [
    "### Check the distribution of years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12189dbb-f4a1-41dc-bed0-75b9cfc875a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Journey_year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14060dff-7180-47bc-8812-596e3f26ff80",
   "metadata": {},
   "source": [
    "### Check the distribution of Duration of Total Minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a7da8d-7359-4456-b158-30af7e59b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duration Total Minutes Distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(data['Duration_total_mins'], bins=30, kde=True, color='skyblue')\n",
    "plt.title('Duration Total Minutes Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf56628-5906-4763-9897-7c9522793010",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb584301-dfbb-4d3b-8939-8e57d4bfa684",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=nominal_features, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b4573-6d6f-4fc5-a266-d506e89f07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_columns = data.select_dtypes(include=[\"bool\"]).columns\n",
    "data[bool_columns] = data[bool_columns].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f860e467-da11-4132-91dd-c947244f9439",
   "metadata": {},
   "source": [
    "## Check for Homoscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2ce237-73ca-4b19-b39d-17be8a12688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = contineous_features + discrete_features + ordinal_features\n",
    "for feature in numeric_features:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.regplot(x=feature, y='Price', data=data, scatter_kws={'s': 50}, line_kws={'color': 'red', 'lw': 2})\n",
    "    plt.title(f'{feature} vs Price with Regression Line')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Price')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b52926-cba0-4eb8-8f84-9f05a956dca7",
   "metadata": {},
   "source": [
    "## Normalization and standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d2be5-4c57-4f31-a2f6-9d4ad8d935ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "data['Duration_total_mins'] = scaler.fit_transform(data[['Duration_total_mins']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7676cd5-44ca-44b3-a05d-4c08a928a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [feature for feature in ordinal_features if feature != \"total_stops\"]\n",
    "ordinalEncoder = OrdinalEncoder()\n",
    "standardScalar = StandardScaler()\n",
    "data[selected_features] = ordinalEncoder.fit_transform(data[selected_features])\n",
    "data[selected_features] = standardScalar.fit_transform(data[selected_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f356c93-29e2-447e-8cd7-ffdc8efe3b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Total_Stops', 'Journey_day', 'Journey_month', 'Journey_year', 'Dep_hour', 'Dep_min', 'Duration_total_mins']] = data[['Total_Stops', 'Journey_day', 'Journey_month', 'Journey_year', 'Dep_hour', 'Dep_min', 'Duration_total_mins']].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f949d3e-fc18-4717-9bbe-06bfc0ab5317",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ee33c0-a2e1-43d8-bbff-ef6cc769052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"Price\", axis=1)\n",
    "Y  = data[\"Price\"]\n",
    "x = sm.add_constant(X)\n",
    "est = sm.OLS(Y, x).fit()\n",
    "print(est.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ab3a5-66ff-49cf-8397-5f5c3703f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['Price']) \n",
    "y = data['Price'] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8df3e48-dbc3-42c1-8a88-ce8caa2e76b5",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced8135-1c65-4124-8f31-f03211ed53cc",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869ebd4-9656-4709-9488-7da92a58c624",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc488958-3673-4c93-80c2-1b74d66217bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter distribution for Random Forest Regressor\n",
    "pca = PCA()\n",
    "param_dist = {\n",
    "    'pca__n_components': [None, 0.9, 0.95, 0.99],  # PCA number of components to test (None for no reduction)\n",
    "     'rf__n_estimators': randint(100, 500),  # Use a higher number of trees\n",
    "    'rf__max_features': ['sqrt'],  # Limit the number of features to consider\n",
    "    'rf__max_depth': randint(5, 30),  # Reduce max depth\n",
    "    'rf__min_samples_split': randint(5, 15),  # More samples to split\n",
    "    'rf__min_samples_leaf': randint(3, 15),  # More samples per leaf\n",
    "    'rf__bootstrap': [True]  # Use bootstrap sampling\n",
    "}\n",
    "\n",
    "# Initialize Random Forest Regressor (without specifying hyperparameters)\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "pipeline = Pipeline([\n",
    "    ('pca', pca),  # Apply PCA for dimensionality reduction\n",
    "    ('rf', rf)  # Apply Random Forest Regressor\n",
    "])\n",
    "\n",
    "# Initialize RandomizedSearchCV with the parameter distribution\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=150,  # Number of different hyperparameter combinations to sample\n",
    "    cv=kfold,  # 5-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',  # Use MSE as the scoring metric\n",
    "    verbose=1,\n",
    "    random_state=42,  # Fix random seed for reproducibility\n",
    "    n_jobs=-1  # Use all cores for faster computation\n",
    ")\n",
    "\n",
    "# Train the model using RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from the random search\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "print(\"Best Param: \", best_params)\n",
    "print(\"Best Score: \", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae5596d-448b-4c8e-97d0-e1300d2e3768",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a3153-dec8-4ddf-b3e0-8862aac24e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Model Performance\n",
    "print(\"\\nRandom Forest Model Performance (RandomizedSearchCV):\")\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "print(\"Root Mean Squared Error: \", rmse)\n",
    "print(f\"R-Squared Value: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40abb364-c2da-4971-9d6c-ac10c3fc790f",
   "metadata": {},
   "source": [
    "### Visuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1292d2a-f193-4185-bc8a-73218aafb551",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ResidualsPlot(best_model)\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show()                 # Finalize and render the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4068fcdd-3e0b-4e55-8cc8-3a14eb653dec",
   "metadata": {},
   "source": [
    "## SVC Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff62cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SVR model\n",
    "svr = SVR()\n",
    "pca = PCA()\n",
    "# Define the parameter grid for SVR\n",
    "param_grid = {\n",
    "    'pca__n_components': [None, 0.9, 0.95, 0.99],  # PCA number of components to test (None for no reduction)\n",
    "    'svr__C': [0.1, 1.0, 10.0, 100.0],           # Regularization parameter\n",
    "    'svr__epsilon': [0.01, 0.1, 0.5, 1.0],      # Margin of tolerance\n",
    "    'svr__kernel': ['linear', 'rbf', 'poly'],    # Kernel types\n",
    "    'svr__degree': [2, 3, 4],                   # Degree for polynomial kernel\n",
    "    'svr__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('pca', pca),  # Apply PCA for dimensionality reduction\n",
    "    ('svr', svr)  # Apply Random Forest Regressor\n",
    "])\n",
    "\n",
    "\n",
    "grid_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=150,\n",
    "    cv=kfold,  # 10-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',  # Use MSE as the metric\n",
    "    verbose=1,\n",
    "    random_state=42,  # For reproducibility\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters: \", best_params)\n",
    "print(\"Best neg_mean_squared_error: \", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53ac1d6-f676-4c11-a2f7-d067b0fef011",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c1061b-08f2-4ffd-8de6-5f9f8d2a5806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "# Model Performance\n",
    "print(\"\\nLinear Regression Performance:\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R-Squared Value: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7963b5b6-4315-4424-9c3d-7ac34b994f2c",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b818e4-44d3-4f9c-94b1-9c8fed027ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ResidualsPlot(best_model)\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ab4ff7-1600-4458-911b-276f04f62368",
   "metadata": {},
   "source": [
    "## XgBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83db901-1617-4b92-880a-19b65bdbe70a",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd046166-5c8c-4cfd-9b61-e07afcc2a984",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBRegressor()\n",
    "pca = PCA()\n",
    "param_grid = {\n",
    "    'pca__n_components': [None, 0.9, 0.95, 0.99],  # PCA number of components to test (None for no reduction)\n",
    "    'xgb__max_depth': range(4, 10, 2),\n",
    "    'xgb__min_child_weight': range(3, 6, 2),\n",
    "    'xgb__gamma': [i / 10.0 for i in range(2, 5)],\n",
    "    'xgb__subsample': [i / 10.0 for i in range(7, 10)],\n",
    "    'xgb__colsample_bytree': [i / 10.0 for i in range(4, 10)],\n",
    "    'xgb__reg_alpha': [0, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('pca', pca),  # Apply PCA for dimensionality reduction\n",
    "    ('xgb', xgb)  # Apply Random Forest Regressor\n",
    "])\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=pipeline, param_distributions=param_grid,\n",
    "                                   n_iter=150, scoring='neg_mean_squared_error',\n",
    "                                   cv=kfold, verbose=1, n_jobs=-1, random_state=42)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "best_params = random_search.best_params_\n",
    "best_model = random_search.best_estimator_\n",
    "best_score = random_search.best_score_\n",
    "print(\"Best param: \", best_params)\n",
    "print(\"Best Score: \", best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d66a4c-d2ce-432c-ad6e-cb8129668de1",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c0c281-adce-41e9-92e4-40e7520d0217",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "# Model Performance\n",
    "print(\"\\nLinear Regression Performance:\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R-Squared Value: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d9696a-1860-4208-8acd-ae5a69c15b39",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca6ebc-6e8d-44a6-b29e-c613f2d345cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ResidualsPlot(best_model)\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show()                 # Finalize and render the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79296ea-39eb-4ab3-a1fe-f88903293a90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
